{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2023/)    \n",
    "## 1MS041, 2023 \n",
    "&copy;2023 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 13. High-Dimensional Space\n",
    "\n",
    "## Topics\n",
    "\n",
    "1. Tail inequalities\n",
    "- Using LLN to estimate the volume of the unit ball\n",
    "- The exact volume of the unit ball\n",
    "- Volume near the equator\n",
    "- Uniform at random from the unit ball\n",
    "- Gaussian Annulus theorem\n",
    "- Random projections (Johnson Lindenstrauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The LLN and volume of the unit ball in $d$ dimensions\n",
    "\n",
    "The Law of Large Numbers can be used to obtain some information about the volume of a unit ball in $d$ dimensions. To see this let us start with $z$ a d-dimensional random variable where each coordinate is Gaussian with variance $\\sigma^2 = 1/(2\\pi)$ (you will se why in a bit), then we have\n",
    "$$\n",
    "    f(x) = \\frac{1}{(2\\pi)^{d/2} \\sqrt{|\\Sigma|}} \\exp(-\\frac{1}{2} x^T \\Sigma^{-1} x)\n",
    "$$\n",
    "and $|\\Sigma| = det(\\Sigma) = det \\left (\\left ( \\frac{1}{2\\pi} \\right ) I \\right ) = \\left ( \\frac{1}{2\\pi} \\right )^d$ and thus\n",
    "$$\n",
    "    f(x) = \\exp(-\\pi |x|^2)\n",
    "$$\n",
    "from this we get that $f(0) = 1$ and $f(x) \\geq e^{-\\pi}$ when $|x| \\leq 1$, or $x \\in B_1(0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now denote $Z = (X_1,X_2,\\ldots, X_n)$, then we see that $|Z|^2 = \\sum_i |X_i|^2$ and that all $X_i$ have the same distribution, as such we can apply the LLN and get\n",
    "$$\n",
    "    P\\left ( \\left | \\frac{|Z|^2}{d} - \\frac{1}{2\\pi}\\right | \\geq \\epsilon \\right ) \\leq \\frac{Var(|X_1|^2)}{d \\epsilon^2} = \\frac{c}{d \\epsilon^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In particular we get for $\\epsilon < 1/(2\\pi)$\n",
    "$$\n",
    "    \\frac{|B_1|}{e^{\\pi}} \\leq P(Z \\in B_1) \\leq P\\left ( \\left | |Z|^2 - \\frac{d}{2\\pi}\\right | \\geq d\\epsilon \\right ) \\leq \\frac{c}{d \\epsilon^2}\n",
    "$$\n",
    "\n",
    "Thus the volume of the unit ball will decrease with dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lets try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dimensions = np.arange(1,20) # Lets try dimension 1 to 20\n",
    "numberOfExperiments = 10000 # Using 10000 experiments to estimate probability\n",
    "scale = np.sqrt(1/(2*np.pi))\n",
    "normals = [np.random.normal(size=(n,numberOfExperiments))*scale for n in dimensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norms = [np.linalg.norm(x,axis=0) for x in normals] # Compute the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probInsideUnitBall = [np.mean(x < 1) for x in norms] # Estimate probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probInsideUnitBall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(dimensions,probInsideUnitBall)\n",
    "# The 20 is just for scale, recall the constant c \n",
    "# # and the arbitrary \\epsilon in our estimate\n",
    "plt.plot(dimensions,20/(np.pi*dimensions),color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We did a fairly poor job at capturing the behavior, the actual volume seems to be much smaller than our estimate. Lets inspect this further in the coming part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lets also look at the average length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meanNorm = np.mean(np.array(norms),axis=1)\n",
    "plt.plot(dimensions,meanNorm)\n",
    "plt.plot(dimensions,np.sqrt(dimensions/(2*np.pi)),color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This seems fairly spot on, interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The geometry of high dimension\n",
    "\n",
    "The scaling property of volume. Lets say we have a cube centered at the origin, namely the cube can be written as $Q = [-l,l]^d$ where $d$ is the dimension, the volume is the product of the side-lengths and thus $Vol(Q) = (2l)^d$. Scaling each side of the cube by $(1-\\epsilon)$ where $\\epsilon$ is a small number gives us that the volume also scales with $(1-\\epsilon)^d$, this gives us the formula\n",
    "$$\n",
    "    Vol((1-\\epsilon)Q)=(1-\\epsilon)^d Vol(Q)\n",
    "$$\n",
    "Lets divide this equation by the volume of $Q$, we get\n",
    "$$\n",
    "    \\frac{Vol((1-\\epsilon)Q}{Vol(Q)} = (1-\\epsilon)^d \\to 0\n",
    "$$\n",
    "as $d \\to \\infty$. The conclusion is that most of the volume is located close to the surface of the cube. The same argument holds true for balls as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(dimensions,np.power(1-0.1,dimensions))\n",
    "plt.plot(dimensions,np.exp(-0.1*dimensions),color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Based on the above, we can choose $\\epsilon = 1/d$ which gives us that most of the volume is contained in the annulus below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"images/Annulus.png\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  Some preliminaries\n",
    "\n",
    "Before we begin we need some preliminaries, i.e. we need to recall how to perform change of variables in integration as well as the spherical coordinate system and its Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Utils import showURL\n",
    "showURL('https://en.wikipedia.org/wiki/Change_of_variables#Coordinate_transformation',300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showURL('https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates',300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Properties of the unit ball\n",
    "\n",
    "No discussion about high dimensional geometry is complete without discussion about the volume of the unit ball. Computing the volume of the unit ball requires some effort, but I think we should walk through it, even though its in the book as some comments are in order. Basically the computation follows the following structure\n",
    "\n",
    "1. Write the volume of the unit ball as an integral of the constant function $1$ over the unit ball\n",
    "2. Use a radial coordinate system to rewrite that integral so that we get the integral over the surface of a unit ball instead.\n",
    "3. Compute the integral of the Gaussian kernel in two ways, one using the fact that $\\exp(|x|^2) = \\exp(|x_1|^2)\\exp(|x_2|^2)\\ldots \\exp(|x_d|^2$, the second one using radial coordinates\n",
    "4. The radial part of the Gaussian integral gives rise to the Gamma function, which is a generalization of the factorial, the spherical part is just the area of the unit sphere (which is the one we are after)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. $$ |B_1| = \\int_{B_1} dx $$\n",
    "2. $$ \\int_{B_1}dx = \\int_{S^d} \\int_0^1 \\left | \\frac{dx}{dr} \\right | dr d\\Omega $$\n",
    "where $\\left | \\frac{dx}{dr} \\right |$ is the Jacobian of the change of variables and $d\\Omega$ is the surface element on the unit sphere $S^d$.\n",
    "$$\\left | \\frac{dx}{dr} \\right | = r^{d-1}$$\n",
    "The conclusion is that \n",
    "$$ \\int_{B_1}dx = \\int_{S^d} \\int_0^1 \\left | \\frac{dx}{dr} \\right | dr d\\Omega = \\frac{A(d)}{d}$$\n",
    "where $A(d) = \\int_{S^d} d\\Omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. This is where we use the Gaussian kernel trick, first note that\n",
    "$$\n",
    "    \\int_{-\\infty}^{\\infty} e^{-x^2}dx = \\sqrt{\\pi}\n",
    "$$\n",
    "The normal random variable has a normalizing factor which is $1/\\sqrt{pi}$!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sympy import var, integrate\n",
    "from sympy.core.numbers import oo\n",
    "from sympy.functions import exp\n",
    "x = var('x')\n",
    "integrate(exp(-x**2),(x,-oo,oo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$$\n",
    "    \\int_{\\mathbb{R}^d} e^{-|x|^2} dx = \\int_{\\mathbb{R}^d} \\prod_{i=1}^d e^{-x_i^2} dx = \\prod_{i=1}^d \\int_{\\mathbb{R}^d} e^{-x_i^2} dx_i = \\pi^{d/2}\n",
    "$$\n",
    "Let us compute the same integral again, but this time using spherical coordinates\n",
    "$$\n",
    "    \\int_{\\mathbb{R}^d} e^{-|x|^2} dx = \\int_{S^d} \\int_0^\\infty e^{-r^2} r^{d-1} dr d\\Omega = A(d) \\int_0^\\infty e^{-r^2} r^{d-1} dr\n",
    "$$\n",
    "now doing the change of variables $t = r^2$ we get $dt = 2 r dr$ and thus\n",
    "$$\n",
    "    \\int_0^\\infty e^{-r^2} r^{d-1} dr = \\int_0^\\infty e^{-t} t^{\\frac{d-1}{2}} \\frac{1}{2\\sqrt{t}} dt = \n",
    "    \\frac{1}{2} \\int_0^\\infty e^{-t} t^{\\frac{d}{2}-1} dt = \\frac{1}{2} \\Gamma\\left ( \\frac{d}{2}\\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the above, the $\\Gamma$ function is a well known function that has a bunch of interesting properties, but the most important is that, if $k$ is an integer then $\\Gamma(k) = (k-1)!$.\n",
    "\n",
    "Finally we come to the conclusion that the volume of the unit ball in $d$ dimensions is\n",
    "\n",
    "$$\n",
    "    V(d) = \\frac{2 \\pi^{\\frac{d}{2}}}{d \\Gamma(\\frac{d}{2})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import factorial\n",
    "plt.scatter(dimensions,2*np.power(np.pi,dimensions/2)/(dimensions*factorial(dimensions/2-1)))\n",
    "# points([(d,2*pi^(d/2)/(d*factorial(d/2-1))) for d in range(2,20,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showURL('https://en.wikipedia.org/wiki/Gamma_function',400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generating points uniformly at random from a ball\n",
    "\n",
    "Lets say that we want to generate a uniformly at random variable on the unit circle. One suggestion would be to generate two coordinates $X$ and $Y$ from $\\text{Uniform}(-1,1)$ and then projecting $(X,Y)$ onto the unit circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XY = np.random.uniform(-1,1,size=(10000,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(XY[:,0],XY[:,1],alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XY = XY / np.linalg.norm(XY,axis=1).reshape(-1,1)\n",
    "plt.scatter(XY[:,0],XY[:,1])\n",
    "plt.gca().set_aspect('equal') # To make sure that a circle is a circle and not an ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=plt.hist(np.arctan2(XY[:,1],XY[:,0]),bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OK, that is not uniform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showURL('https://numpy.org/doc/stable/reference/generated/numpy.arctan2.html',300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What could we do? Well, we can discard everything outside the unit square and project those to the circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XY = np.random.uniform(-1,1,size=(10000,2))\n",
    "XY_inCircle = XY[np.linalg.norm(XY,axis=1) < 1]\n",
    "\n",
    "XY_inCircle = XY_inCircle / np.linalg.norm(XY_inCircle,axis=1).reshape(-1,1)\n",
    "\n",
    "plt.scatter(XY_inCircle[:,0],XY_inCircle[:,1])\n",
    "plt.gca().set_aspect('equal') # To make sure that a circle is a circle and not an ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=plt.hist(np.arctan2(XY_inCircle[:,1],XY_inCircle[:,0]),bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OK, much better! But this does not work in higher dimensions, why?\n",
    "Well we already showed that the volume of the unit ball decreases rapidly with dimension while the volume of the cube is $2^d$, so the probability of being inside the unit ball is decreasing very rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Uniform at random on the unit sphere (better version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What was the problem that we had, well basically if we sample from the unit square that distribution is not rotationally symmetric, thus if we sample from a rotationally symmetric distribution then it does not matter, we can just scale any sample to be on the unit circle. A prime example of a rotationally symmetric random variable is the multidimensional Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XY = np.random.normal(size=(10000,2))\n",
    "\n",
    "XY = XY / np.linalg.norm(XY,axis=1).reshape(-1,1)\n",
    "_=plt.hist(np.arctan2(XY[:,1],XY[:,0]),bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## How do we generate uniform at random from the unit ball?\n",
    "\n",
    "In the above we learned how to generate uniform at random from the unit sphere. How can we use this to fill out the entire ball? Perhaps we think that if we take $r \\sim \\mathrm{Unif}([0,1])$ and $\\omega \\sim \\mathrm{Unif}(\\partial B)$, then the final point could be $(r,\\omega) \\in B$? But is this uniform? Well $(r,\\omega) \\mid r$ is uniform on $\\partial B_r$.\n",
    "$$\n",
    "    \\lim_{h \\to 0}\\frac{1}{h} P((r,\\omega) \\in (B_{t+h} \\setminus B_t)) = \\lim_{h \\to 0}\\frac{1}{h} \\int_{(B_{t+h} \\setminus B_t))} f(r,\\omega) dr d\\omega = \\lim_{h \\to 0}\\frac{1}{h}\\int_{t}^{t+h} \\left ( \\int_{\\partial B} f(r,\\omega) d\\omega \\right ) dr = \\int_{\\partial B} f(t,\\omega) d\\omega = f(t)\n",
    "$$\n",
    "for some function $f(t)$ that we will now find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now for a uniform distribution the probability of being in a set is proportional to its volume and hence\n",
    "$$\n",
    "    \\lim_{h \\to 0} \\frac{1}{h} P((r,\\omega) \\in (B_{t+h} \\setminus B_t)) = \\lim_{h \\to 0} \\frac{|B_{t+h} \\setminus B_t|}{h |B|} = c_0 \\lim_{h \\to 0} \\frac{(t+h)^d-s^d}{h} = c_1 t^{d-1}\n",
    "$$\n",
    "hence\n",
    "$$\n",
    "    f(t) = c_2 t^{d-1}\n",
    "$$\n",
    "To find $c_2$ we know that\n",
    "$$\n",
    "    \\int_0^1 f(t) dt = 1 \\implies c_2 = d\n",
    "$$\n",
    "Lets try this in 2-d and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "XY = np.random.normal(size=(100000,2)) # Spherical Gaussian with unit variance in each coordinate in R^2\n",
    "\n",
    "XY = XY / np.linalg.norm(XY,axis=1).reshape(-1,1) # Make all vector unit length\n",
    "\n",
    "r = np.random.uniform(size=(XY.shape[0],1)) # Sample the radii uniformly from [0,1]\n",
    "uniform_ball = np.sqrt(r)*XY # Consider the correctly scaled radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist2d(uniform_ball[:,0],uniform_ball[:,1],bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Gaussian Annulus theorem\n",
    "\n",
    "Remember that for a d-dimensional spherical Gaussian with standard deviation $1$ in each dimension satisfies\n",
    "$$\n",
    "    E[|x|^2] = d.\n",
    "$$\n",
    "Thus one could expect that $|x|$ concentrates around $\\sqrt{d}$. The next theorem makes this rigorous, as well as providing tail bounds.\n",
    "\n",
    ">For a $d$-dimensional spherical Gaussian with unit variance in each direction, for any $\\beta < \\sqrt{d}$, all but at most $3 e^{-c\\beta^2}$ of the probability mass lies within the annulus\n",
    "> $$\\sqrt{d} - \\beta \\leq |x| \\leq \\sqrt{d} + \\beta$$\n",
    "> where $c$ is a fixed positive constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Proof\n",
    "\n",
    "The proof follows from a concentration inequality for sums of random variables, see the master tails bound. Whenever we have sums of random variables usually have a lot of cancellation, i.e. all the variables are not big at the same time. The more variables the smaller the tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 10^2\n",
    "n_samples = 10000\n",
    "dGaussian = np.random.normal(size=(n_samples,d))\n",
    "dGaussianNorm = np.linalg.norm(dGaussian,axis=1)\n",
    "_=plt.hist(dGaussianNorm,bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## K-Nearest Neighbors Algorithm\n",
    "\n",
    "The $k$-nearest neighbor algorithm is a particularly simple method that is nonparametric and in fact requires no training, or, to put it bluntly, it uses training data as a base of the model, but does not perform any computation for training. So what is actually going on here?\n",
    "\n",
    "* For **classification** we proceed as follows: Let us say we are given a dataset $X$ with labels $y$, whenever we are given a new point $x$ and wish to predict which class this point corresponds to, we simply look at the $k$-nearest neighbors and find the most common class within those.\n",
    "* For **regression** we proceed as follows: Let us say we are given a dataset $X$ with targets $y$, whenever we are given a new point $x$ and wish to predict which target value this point corresponds to, we simply look at the $k$-nearest neighbors and take the average of those points target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How do we compute the $k$ nearest neighbours? First we start with a distance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For simplicity we will construct a classification dataset with two features and two classes\n",
    "\n",
    "X_data = np.array([[ 0.36731074, -0.26731719, -0.72426635, -0.54930901, -0.47614201,\n",
    "                     0.1327083 ,  1.30847308,  0.19501328, -0.99805696,  0.14979754],\n",
    "                   [ 0.37311239,  0.56515267, -0.1917514 , -0.14742026,  0.2890942 ,\n",
    "                    -0.02590534, -0.53987907,  0.70816002, -0.92185638,  0.92011316],\n",
    "                   [-0.30774856,  0.84222474,  1.36755686,  0.2035808 ,  0.91745894,\n",
    "                     2.39470366, -0.11227247, -0.36218045,  0.96482992,  0.94849202],\n",
    "                   [-0.66770717, -0.93943336,  1.12547913, -0.48933722, -0.21269764,\n",
    "                    -0.80459114, -0.33914025,  0.31216994,  1.78198367, -0.54211499],\n",
    "                   [ 0.40071561,  0.40020999, -2.30125766, -0.33763234, -0.7319695 ,\n",
    "                     1.25647226,  0.66023155, -0.35087189, -1.34430587, -1.96996738]])\n",
    "\n",
    "y_data = np.array([0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let $S = \\{X_1,X_2,\\ldots,X_n\\}$ where each $X_i \\in \\mathbb{R}^d$, then the distance matrix is denoted as\n",
    "$$\n",
    "    D_{mat}(S) = \\begin{bmatrix}\n",
    "        |X_1-X_1| & |X_1-X_2| & \\ldots & |X_1-X_n| \\\\\n",
    "        |X_2-X_1| & |X_2-X_2| & \\ldots & |X_2-X_n| \\\\\n",
    "        \\vdots \\\\\n",
    "        |X_n-X_1| & |X_n-X_2| & \\ldots & |X_n-X_n| \\\\\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XX = X_data.reshape(X_data.shape[0],1,X_data.shape[1])-X_data.reshape(1,X_data.shape[0],X_data.shape[1])\n",
    "distance_matrix = np.linalg.norm(XX,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How do we find the k-nearest neighbors?\n",
    "\n",
    "Lets make an example, let $k= 2$ and consider $X_1$ in the above, we see that the two closest points are $X_2,X_5$.\n",
    "For $X_2$ we have $X_1,X_3$ etc.\n",
    "\n",
    "But, if all we want to do is to compute the k-nearest neighbors from $X$ to $S$, we can compute $|X-X_i|$ for all i and sort the list (for instance), then choose the bottom $k$.\n",
    "\n",
    "We can think of $S$ as a database and $X$ as a query:\n",
    "> What is the k nearest neighbors of X?\n",
    "\n",
    "How many computations are needed for such a query? If we use the technique above then we get\n",
    "1. $C n d$, where I think $C$ is 4, to compute the distances\n",
    "2. $n \\log n$ to sort\n",
    "\n",
    "Total $C nd + n \\log n$, we cannot really make $n$ smaller, but what if we could make $d$ smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Random Projection and Johnson-Lindenstrauss Lemma\n",
    "### Random Projection Theorem\n",
    "> Let $v$ be a fixed vector in $\\mathbb{R}^d$, then there exists a constant $c > 0$ such that if we for a fix $\\epsilon \\in (0,1)$\n",
    "> pick $k$ Gaussian vectors $u_1,\\ldots, u_k \\in \\mathbb{R}^d$ and form the function\n",
    "> $$f(v) = (u_1 \\cdot v, \\ldots, u_k \\cdot v): \\mathbb{R}^d \\to \\mathbb{R}^k,$$\n",
    "> then\n",
    "> $$P\\left (\\left | |f(v)| - \\sqrt{k} |v| \\right |  \\geq \\epsilon \\sqrt{k}|v| \\right ) \\leq 3 e^{-ck\\epsilon^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets simulate this lemma and see what happens. Try increasing d and see what happens to the distribution, then increase k and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets simulate\n",
    "import numpy as np\n",
    "from ipywidgets import interact, IntSlider\n",
    "@interact\n",
    "def _(d=IntSlider(100,100,2000,100),k=IntSlider(2,2,100,1)):\n",
    "    np.random.seed(1)\n",
    "    v_pre = np.random.normal(size=d)\n",
    "    v = v_pre / np.linalg.norm(v_pre)\n",
    "\n",
    "    print(\"v has length: %.2f\" % np.linalg.norm(v))\n",
    "\n",
    "    num_simulations = 300\n",
    "\n",
    "    error = []\n",
    "\n",
    "    for i in range(num_simulations):\n",
    "        uis = np.random.normal(size=(k,d))\n",
    "        f = uis@v\n",
    "        error.append(abs(np.linalg.norm(f)-np.linalg.norm(v)*np.sqrt(k))/(np.linalg.norm(v)*np.sqrt(k)))\n",
    "    _=plt.hist(error)\n",
    "    # P=histogram(error)\n",
    "    # P.xmax(1)\n",
    "    # P.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### What can you get from the above simulation?\n",
    "Is this a good way of projecting? How high dimension do we really need?\n",
    "\n",
    "### You try at home\n",
    "Can you find a better way of using numpy to perform the simulation above faster? So that we quickly can simulate 10000 examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Proof\n",
    "\n",
    "Lets consider unit vectors $v$ as in the simulation above. Note now that\n",
    "$$\n",
    "    u_i \\cdot v = \\sum_{j=1}^d (u_i)_j v_j\n",
    "$$\n",
    "each $(u_i)_j$ is N(0,1) and independent of each other, as such $u_i \\cdot v$ is N(0,1). From this follows that $f(v)$ is a spherical Gaussian in $\\mathbb{R}^k$, as such the theorem follows from the Gaussian Annulus theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Johnson-Lindenstrauss Lemma\n",
    ">For any $0 < \\epsilon < 1$  and any integer $n$, let $k \\geq \\frac{3}{c\\epsilon^2} \\ln n$ where $c$ is as in the random projection theorem. For any set of $n$ points $\\{v_1,\\ldots,v_n\\}$ in $\\mathbb{R}^d$ then the random projection defined in the random projection theorem satisfies\n",
    ">$$P((1-\\epsilon) \\sqrt{k} |v_i-v_j| \\leq |f(v_i-v_j)| \\leq (1+\\epsilon) \\sqrt{k} |v_i-v_j|: \\forall i, j) \\geq 1-\\frac{3 }{2 n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Proof\n",
    "For any fixed $v_i,v_j$ we can apply the random projection theorem and obtain that\n",
    "$$\n",
    "    P\\left (\\left | |f(v_i-v_j)| - \\sqrt{k} |v_i-v_j| \\right | > \\epsilon |v_i-v_j|\\right) \\leq 3e^{-c k \\epsilon^2}\n",
    "$$\n",
    "There are $\\binom{n}{2} < n^2/2$ pairs to consider, so  using the union bound we get\n",
    "$$\n",
    "    P\\left (\\left | |f(v_i-v_j)| - \\sqrt{k} |v_i-v_j| \\right | > \\epsilon |v_i-v_j|: \\exists i, j\\right)  \\leq \\frac{3 n^2}{2} e^{-c k \\epsilon^2}\n",
    "$$\n",
    "Choose $k \\geq \\frac{3}{c \\epsilon^2}$ then\n",
    "$$\n",
    "    P\\left (\\left | |f(v_i-v_j)| - \\sqrt{k} |v_i-v_j| \\right | > \\epsilon |v_i-v_j|: \\exists i, j\\right)  \\leq \\frac{3 }{2 n}\n",
    "$$\n",
    "which gives\n",
    "$$\n",
    "    P((1-\\epsilon) \\sqrt{k} |v_i-v_j| \\leq |f(v_i-v_j)| \\leq (1+\\epsilon) \\sqrt{k} |v_i-v_j|: \\forall i, j) \\geq 1-\\frac{3 }{2 n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### YouTry:\n",
    "Perform a simulation of the Johnson-Lindenstrauss lemma. You can use the following set of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d = 1000\n",
    "n = 100\n",
    "vis = np.random.normal(size=(n,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You will need to find a way to fairly quickly loop over all pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exam example problem\n",
    "\n",
    "The goal of this problem is to implement your own random projection function, i.e. same as in the random projection theorem and Johnson Lindenstrauss. Below you will find a function template, as always fill in XXX.\n",
    "\n",
    "Some notes, the input will be a numpy array and the output should also be a numpy array. The format for the input is `(n_samples, d)` and the output should be of the form `(n_samples, k)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do not rename the function below just replace what is XXX\n",
    "def random_projection(X,d,k):\n",
    "    '''Perform a random projection of the dataset X from \n",
    "    d-dimensions into k-dimensions'''\n",
    "    assert X.shape[1] == XXX, \"The array X does not have the shape (n_samples,d)\"\n",
    "    XXX\n",
    "    XXX\n",
    "    return XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# small test to see that you output the correct shape\n",
    "import numpy as np\n",
    "X_test = np.array([[1,1],[2,1],[3,1],[4,1]])\n",
    "try:\n",
    "    assert(random_projection(X_test,2,1).shape == (X_test.shape[0],1))\n",
    "    print(\"You are outputting the correct dimension, hopefully it is correct.\")\n",
    "except AssertionError:\n",
    "    print(\"Try again! and make sure you are producing an output of (n_samples,k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_points=0\n",
    "from math import sqrt\n",
    "\n",
    "try:\n",
    "    A = np.array([1,0]).reshape(1,-1)\n",
    "    B = np.array([0,1]).reshape(1,-1)\n",
    "    n_test = 1000\n",
    "    SampA = np.array([random_projection(A,2,1).reshape(-1)[0] for i in range(n_test)])\n",
    "    assert np.abs(SampA.mean()) < 8*1/sqrt(n_test)\n",
    "    assert np.abs(SampA.var()) < 1+8*1/sqrt(n_test)\n",
    "    SampB = np.array([random_projection(B,2,1).reshape(-1)[0] for i in range(n_test)])\n",
    "    assert np.abs(SampB.mean()) < 8*1/sqrt(n_test)\n",
    "    X_test2 = np.zeros((100,2300))\n",
    "    assert(random_projection(X_test2,2300,30).shape == (X_test2.shape[0],30))    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"You have failed to pass the test for this problem. Try again..\")\n",
    "    print(\"You are either not randomizing correctly or you are not projecting to the right space\")\n",
    "else:\n",
    "    print(\"You have passed the test for this problem, congratulations!\")\n",
    "    local_points += 1\n",
    "finally:\n",
    "    print(\"The number of points you have scored for this problem so far = \"+str(local_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## High dimensional data-set and nearest neighbor search\n",
    "\n",
    "Use your random projection implementation from the assignment above and explore this dataset.\n",
    "\n",
    "### Description\n",
    "\n",
    "Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.\n",
    "\n",
    "Science, VOL 286, pp. 531-537, 15 October 1999. Web supplement to the article\n",
    "\n",
    "T.R. Golub, D. K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. P. Mesirov, H. Coller, M. L. Loh, J. R. Downing, M. A. Caligiuri, C. D. Bloomfield, E. S. Lander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "features = []\n",
    "labels = []\n",
    "with open('data/leukemia.csv',mode='r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header=next(f)\n",
    "    for row in reader:\n",
    "        features.append(np.array(row[:-1],dtype=float))\n",
    "        labels.append((row[-1] == 'ALL')*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.stack(features,axis=0)\n",
    "Y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For those who cannot install sklearn we can use the implementation found at https://github.com/mavaladezt/kNN-from-Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def knn_distances(xTrain,xTest,k):\n",
    "    \"\"\"\n",
    "    Finds the k nearest neighbors of xTest in xTrain.\n",
    "    Input:\n",
    "    xTrain = n x d matrix. n=rows and d=features\n",
    "    xTest = m x d matrix. m=rows and d=features (same amount of features as xTrain)\n",
    "    k = number of nearest neighbors to be found\n",
    "    Output:\n",
    "    dists = distances between all xTrain and all XTest points. Size of n x m\n",
    "    indices = k x m matrix with the indices of the yTrain labels that represent the point\n",
    "    \"\"\"\n",
    "    #the following formula calculates the Euclidean distances.\n",
    "    import numpy as np\n",
    "    distances = -2 * xTrain@xTest.T + np.sum(xTest**2,axis=1) + np.sum(xTrain**2,axis=1)[:, np.newaxis]\n",
    "    #because of float precision, some small numbers can become negatives. Need to be replace with 0.\n",
    "    distances[distances < 0] = 0\n",
    "    distances = distances**.5\n",
    "    indices = np.argsort(distances, 0) #get indices of sorted items\n",
    "    distances = np.sort(distances,0) #distances sorted in axis 0\n",
    "    #returning the top-k closest distances.\n",
    "    return indices[0:k,:], distances[0:k,:]\n",
    "\n",
    "def knn_predictions(xTrain,yTrain,xTest=None,k=3):\n",
    "    \"\"\"\n",
    "    Uses xTrain and yTrain to predict xTest.\n",
    "    Input:\n",
    "    xTrain = n x d matrix. n=rows and d=features\n",
    "    yTrain = n x 1 array. n=rows with label value\n",
    "    xTest = m x d matrix. m=rows and d=features (same amount of features as xTrain)\n",
    "    k = number of nearest neighbors to be found\n",
    "    Output:\n",
    "    predictions = predicted labels, ie preds(i) is the predicted label of xTest(i,:)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    if (xTest == None):\n",
    "        xTest = xTrain\n",
    "        \n",
    "    indices, distances = knn_distances(xTrain,xTest,k)\n",
    "    yTrain = yTrain.flatten()\n",
    "    rows, columns = indices.shape\n",
    "    predictions = list()\n",
    "    for j in range(columns):\n",
    "        temp = list()\n",
    "        for i in range(rows):\n",
    "            cell = indices[i][j]\n",
    "            temp.append(yTrain[cell])\n",
    "        predictions.append(max(temp,key=temp.count)) #this is the key function, brings the mode value\n",
    "    predictions=np.array(predictions)\n",
    "    return predictions\n",
    "\n",
    "def score(prediction,true_values):\n",
    "    return np.sum(prediction == true_values)/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score(knn_predictions(X,Y,k=5),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From this we see that there is a high relation between which items are close and what is the class. Lets check how long time the query takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "score(knn_predictions(X,Y,k=5),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets now project this to a smaller dimensional space, try different values and rerun to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_projection(X,d,k):\n",
    "    '''Perform a random projection of the dataset X from \n",
    "    d-dimensions into k-dimensions'''\n",
    "    assert X.shape[1] == d, \"The array X does not have the shape (n_samples,d)\"\n",
    "    u = np.random.normal(size=(k,d))\n",
    "    uX = X@(u.T)\n",
    "    return uX\n",
    "k = 3\n",
    "X_proj_kd = random_projection(X,X.shape[1],k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score(knn_predictions(X_proj_kd,Y,k=5),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For 1000 dimension there is a high change due to JL lemma that the distances are preserved up to a small epsilon and as such we should not change what is the nearest neighbour, thus the performance can be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "score(knn_predictions(X_proj_kd,Y,k=5),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However we see that the run-time is waay faster. Try also to visualize the projection in dimension 2 and 3, to get a feel for how the projection changes when you rerun the random projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def standardScaler(X_in):\n",
    "    '''Takes an array of shape (n_samples,n_features) and centers and normalizes the data'''\n",
    "    X_out = (X_in-np.mean(X_in,axis=0))/np.std(X_in,axis=0)\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (k in [2,3]):\n",
    "    X_proj_kd_rescale = standardScaler(X_proj_kd)\n",
    "    class0 = X_proj_kd_rescale[Y==0]\n",
    "    class1 = X_proj_kd_rescale[Y==1]\n",
    "    from Utils import scatter3d\n",
    "    fig = scatter3d(class0[:,0],class0[:,1],class0[:,2],c='blue')\n",
    "    fig = scatter3d(class1[:,0],class1[:,1],class1[:,2],c='red',fig=fig)\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "lx_course_instance": "2023",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
