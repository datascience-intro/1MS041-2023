{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2023/)    \n",
    "## 1MS041, 2023 \n",
    "&copy;2023 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concentration and limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an i.i.d. sequence of random variables $X_1,\\ldots,X_n$ each being Bernoulli($1/2$). Then the concept of concentration is telling us that\n",
    "\n",
    "$$\n",
    "    P\\left ( \\left | \\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}(X_i) \\right | > \\epsilon \\right )\n",
    "$$\n",
    "\n",
    "gets smaller as $n$ gets larger. For instance, using Chebychevs inequality we get\n",
    "\n",
    "$$\n",
    "    P\\left ( \\left | \\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}(X_i) \\right | > \\epsilon \\right ) \\leq \\frac{\\mathbb{V}\\left( \\frac{1}{n} \\sum_{i=1}^n X_i \\right )}{\\epsilon^2} = \\frac{\\mathbb{V}\\left( X_0 \\right )}{\\epsilon^2 n}\n",
    "$$\n",
    "\n",
    "We can see that this is at least true in the simulation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c625f4f2eeb4d198bea7b21b40dc507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n', max=50, min=1, step=5), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "from Utils import discrete_histogram\n",
    "import numpy as np\n",
    "@interact \n",
    "def concentration(n=IntSlider(1,1,50,5)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    X = np.random.randint(0,2,size=(n,10000))\n",
    "    means = np.mean(X,axis=0)\n",
    "    print(\"P(mean > mu + 0.3 ) = %.5f <= Chebychev %.5f\" % (np.mean(means > 0.5+0.3),(1/4)/(0.3**2*n)))\n",
    "    discrete_histogram(means,normed=True)\n",
    "    plt.xlim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoeffdings inequality on the other hand is sharper and gives a bound much smaller, namely that\n",
    "\n",
    "$$\n",
    "    P\\left ( \\left |\\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}(X_i) \\right | > \\epsilon \\right ) \n",
    "    \\leq 2e^{-2n \\epsilon^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a565c439f6e24fb18815d8c4802cc327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n', max=50, min=1, step=5), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact \n",
    "def concentration(n=IntSlider(1,1,50,5)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    X = np.random.randint(0,2,size=(n,10000))\n",
    "    means = np.mean(X,axis=0)\n",
    "    print(\"P(mean > mu + 0.3 ) = %.5f <= Hoeffding %.5f\" % (np.mean(means > 0.5+0.3),np.exp(-2*n*0.3**2)))\n",
    "    discrete_histogram(means,normed=True)\n",
    "    plt.xlim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much closer, and in fact very close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using concentration as a measure of confidence\n",
    "\n",
    "We can use concentration as a measure of confidence in the following way. Consider $X_1,\\ldots, X_n$ being i.i.d. sequence of Bernoulli($p$) for some unknown $p$. From the concept of concentration, we would expect that if we have many observations ($n$ large) we could use the empirical mean of the observations as a guess, but note that there is some variability as we saw in the above simulations. So what do we do? We use the concentration inequality to get information how far we can deviate from $p$ in the following way\n",
    "\n",
    "$$\n",
    "    P(\\bar X_n - \\mathbb{E}(\\bar X_n) \\geq \\epsilon) \\leq e^{-2n\\epsilon^2}\n",
    "$$\n",
    "\n",
    "Since $\\mathbb{E}(\\bar X_n) = p$, rearrange and get\n",
    "\n",
    "$$\n",
    "    P(p \\leq \\bar X_n - \\epsilon) \\leq e^{-2n\\epsilon^2}\n",
    "$$\n",
    "\n",
    "The complementary event thus satisfies\n",
    "\n",
    "$$\n",
    "    P(\\bar X_n - \\epsilon < p) \\geq 1-e^{-2n\\epsilon^2}\n",
    "$$\n",
    "\n",
    "We can do the same for the other side (see lecture notes) and we get\n",
    "\n",
    "$$\n",
    "    P(\\bar X_n - \\epsilon < p < \\bar X_n + \\epsilon) \\geq 1-2 e^{-2n\\epsilon^2}.\n",
    "$$\n",
    "\n",
    "If you where now asked to estimate $p$ using $n$ observations and give an interval where you with at least 95% confidence can say contains $p$, then you need to choose $\\epsilon > 0$ such that\n",
    "\n",
    "$$\n",
    "    1-2 e^{-2n\\epsilon^2} \\geq 0.95.\n",
    "$$\n",
    "\n",
    "Smaller $\\epsilon$ gives smaller intervals, so lets choose to have the smallest possible $\\epsilon$ while still obaying the inequality above, i.e. we choose $\\epsilon$ to solve\n",
    "\n",
    "$$\n",
    "    1-2 e^{-2n\\epsilon^2} = 0.95.\n",
    "$$\n",
    "\n",
    "Rearranging we and taking log and then square root we obtain\n",
    "\n",
    "$$\n",
    "    \\epsilon = \\sqrt{-\\frac{1}{2n}\\ln\\left(\\frac{1-0.95}{2}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2dab29fa814c889e15792ff716f3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='n', max=500, min=50, step=50), FloatSlider(value=0.5, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FloatSlider\n",
    "@interact \n",
    "def concentration(n=IntSlider(value=1,min=50,max=500,step=50),p=FloatSlider(value=0.5, min=0,max=1,step=0.1)):\n",
    "    X = np.random.binomial(1,p,size=(n))\n",
    "    means = np.mean(X,axis=0)\n",
    "    epsilon = np.sqrt(-1/(2*n)*np.log((1-0.95)/2))\n",
    "    print(\"95%% confidence interval [%.2f, %.2f] for p=%.2f n=%d\" % (means-epsilon,means+epsilon,p,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a461d8e4a0da464ba95865dd38c6d202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='n', max=5000, min=50, step=50), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FloatSlider\n",
    "@interact \n",
    "def concentration_300(n=IntSlider(value=1,min=50,max=5000,step=50)):\n",
    "    X = np.minimum(np.abs(70+6*np.random.normal(size=(n))),300)\n",
    "    means = np.mean(X,axis=0)\n",
    "    epsilon = np.sqrt(-1/(2*n/(300**2))*np.log((1-0.95)/2))\n",
    "    print(\"95%% confidence interval [%.2f, %.2f] for n=%d\" % (means-epsilon,means+epsilon,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These things are super useful. Lets go back to our sms spam/ham problem and see what we can say there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  0),\n",
       " ('Ok lar... Joking wif u oni...', 0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils import load_sms \n",
    "sms_data = load_sms()\n",
    "sms_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_words=set(['free','prize'])\n",
    "TF10 = {True: 1, False: 0}\n",
    "Z_obs = [TF10[not interesting_words.isdisjoint([word.lower() for word in line[0].split(' ')])] for line in sms_data]\n",
    "Y_obs = [y for x,y in sms_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we computed the mean of Y_obs, which is the mean spam number. This is a Bernoulli random variable with unknown $p$, so we can use our methods above to compute a confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_bernoulli(n,alpha):\n",
    "    return np.sqrt(-1/(2*n)*np.log((alpha)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.116,0.152]\n"
     ]
    }
   ],
   "source": [
    "epsilon = epsilon_bernoulli(len(Y_obs),0.05)\n",
    "mean_Y_obs = np.mean(Y_obs)\n",
    "print(\"[%.3f,%.3f]\" % (mean_Y_obs-epsilon,mean_Y_obs+epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we get that we have provided a prediction as to what is the true probability of getting a spam email.\n",
    "\n",
    "Lets take a look at the conditional probability. Recall that\n",
    "\n",
    "$$\n",
    "    P(Y = 1 \\mid Z = 1) = \\frac{P(Y = 1 \\text{ and } Z = 1)}{P(Z = 1)}\n",
    "$$\n",
    "\n",
    "but this requires you to estimate both things on the right, and give a region for both and finally figure out an interval for the ratio. But, there is an easier way, which sometimes works better. Namely, to look at the random variable $Y \\mid (Z=1)$ which we do by filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mid_Z1 = [y for z,y in zip(Z_obs,Y_obs) if z == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for this you have a certain number of observations and we can use this instead as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.726,0.898]\n"
     ]
    }
   ],
   "source": [
    "epsilon = epsilon_bernoulli(len(Y_mid_Z1),0.05)\n",
    "mean_Y_obs = np.mean(Y_mid_Z1)\n",
    "print(\"[%.3f,%.3f]\" % (mean_Y_obs-epsilon,mean_Y_obs+epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Being clever with Bennetts inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bennett_epsilon(n,b,sigma,alpha):\n",
    "    import scipy.optimize as so\n",
    "    h = lambda u: (1+u)*np.log(1+u)-u\n",
    "    f = lambda epsilon: np.exp(-n*sigma**2/b**2*h(b*epsilon/sigma**2))-alpha/2\n",
    "    ans = so.fsolve(f,0.002)\n",
    "    epsilon = np.abs(ans[0])\n",
    "    print(\"Numerical error\", f(epsilon))\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical error -4.5102810375396984e-17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.457174445041813"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bennett_epsilon(50,300,20,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304246372b1b458194d370cafa77cab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='n', max=5000, min=50, step=50), FloatSlider(value=10.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FloatSlider\n",
    "@interact \n",
    "def concentration_300(n=IntSlider(value=1,min=50,max=5000,step=50),sigma=FloatSlider(value=10, min=1,max=300,step=1)):\n",
    "    X = np.minimum(np.abs(70+6*np.random.normal(size=(n))),300)\n",
    "    means = np.mean(X,axis=0)\n",
    "    epsilon = np.sqrt(-1/(2*n/(300**2))*np.log((1-0.95)/2))\n",
    "    epsilon2 = bennett_epsilon(n,b=300,sigma=sigma,alpha=0.05)\n",
    "    print(\"95%% confidence interval rough [%.2f, %.2f] for n=%d\" % (means-epsilon,means+epsilon,n))\n",
    "    print(\"95%% confidence interval Bennett [%.2f, %.2f] for n=%d\" % (means-epsilon2,means+epsilon2,n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "lx_course_instance": "2023",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
